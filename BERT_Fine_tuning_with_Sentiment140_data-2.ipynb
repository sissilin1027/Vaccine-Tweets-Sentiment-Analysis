{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13e2QHzn737n",
    "outputId": "18f9278a-513d-40ac-c26f-6da69c0a5131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your runtime has 27.4 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ],
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "  print('re-execute this cell.')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoCDM2k-788c",
    "outputId": "70a158e2-550f-434a-f47b-dea5ceaacc2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 25 05:51:47 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   53C    P0    43W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lh-ihqAZOub8",
    "outputId": "29899db2-dd38-4c97-fc91-3f805fc0e3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gm3kSNIrrbdL",
    "outputId": "3c712a1d-10c0-4b2b-e8df-8432e7ee3e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# get the GPU device name\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7NmS-nprdx7",
    "outputId": "aee97c03-8446-4ccd-a7f3-bb30f3cdc1e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "The GPU will be used: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('The GPU will be used:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_SezDqyUnq4r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader,SequentialSampler,RandomSampler,TensorDataset,random_split\n",
    "import os\n",
    "%matplotlib inline\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vqo4zVqcn7iE",
    "outputId": "4ef00337-544d-4a22-e9b7-21c8fc099c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#import training data from gdrive\n",
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUWuSS7Gop3U",
    "outputId": "6414626b-cf16-496c-8d10-71e04ad32338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 6)\n",
      "   Sentiment                                              Tweet\n",
      "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "1          0  is upset that he can't update his Facebook by ...\n",
      "2          0  @Kenichan I dived many times for the ball. Man...\n",
      "3          0    my whole body feels itchy and like its on fire \n",
      "4          0  @nationwideclass no, it's not behaving at all....\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/content/drive/My Drive/training.1600000.processed.noemoticon.csv',  encoding = 'latin-1', header=None)\n",
    "print(data.shape)\n",
    "data = data[[0, 5]]\n",
    "data.columns = ['Sentiment', 'Tweet']\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "cxpljPvguddZ",
    "outputId": "3c04c9ec-6b7b-4a49-eebd-2458630a9166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training tweets: 1,600,000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>630748</th>\n",
       "      <td>0</td>\n",
       "      <td>@DonnieWahlberg gonna see you tomorrow night i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566560</th>\n",
       "      <td>1</td>\n",
       "      <td>i'm in love with my medical card. yes doctor, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580782</th>\n",
       "      <td>0</td>\n",
       "      <td>Tired from volleyball  nice nap sounds good bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520901</th>\n",
       "      <td>1</td>\n",
       "      <td>Good morning! Nice to sleep for about 12hrs.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499038</th>\n",
       "      <td>0</td>\n",
       "      <td>watching Jon and Kate plus 8. FIVE MORE DAYS! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582331</th>\n",
       "      <td>1</td>\n",
       "      <td>@Ayla_F Oh sweetie that's ALWAYS the way. Once...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194070</th>\n",
       "      <td>0</td>\n",
       "      <td>is amused by all the tweets from @Jason_Manfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155720</th>\n",
       "      <td>0</td>\n",
       "      <td>Why do i have to work tomorrow!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707847</th>\n",
       "      <td>0</td>\n",
       "      <td>@Lark_in_Forks [Text] Sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365014</th>\n",
       "      <td>0</td>\n",
       "      <td>@mediatemple No dice on the upgrade - still $2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                              Tweet\n",
       "630748           0  @DonnieWahlberg gonna see you tomorrow night i...\n",
       "1566560          1  i'm in love with my medical card. yes doctor, ...\n",
       "580782           0  Tired from volleyball  nice nap sounds good bu...\n",
       "1520901          1  Good morning! Nice to sleep for about 12hrs.  ...\n",
       "499038           0  watching Jon and Kate plus 8. FIVE MORE DAYS! ...\n",
       "1582331          1  @Ayla_F Oh sweetie that's ALWAYS the way. Once...\n",
       "194070           0  is amused by all the tweets from @Jason_Manfor...\n",
       "155720           0                  Why do i have to work tomorrow!? \n",
       "707847           0                       @Lark_in_Forks [Text] Sorry \n",
       "365014           0  @mediatemple No dice on the upgrade - still $2..."
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 as negative, 1 as positive\n",
    "data.loc[data[\"Sentiment\"] == 4, \"Sentiment\"] = 1\n",
    "print('Number of training tweets: {:,}\\n'.format(data.shape[0]))\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "tRWPxWt-w13D"
   },
   "outputs": [],
   "source": [
    "# clean the dataset\n",
    "import re\n",
    "#remove hashtags\n",
    "hashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\n",
    "#remove @ mentions \n",
    "mentions = re.compile(r\"^@\\S+|\\s@\\S+\")\n",
    "#remove urls\n",
    "urls = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def text_process(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = hashtags.sub(' hashtag', text)\n",
    "    text = mentions.sub(' entity', text)\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "rn-ReFZMw3Xh",
    "outputId": "27eeb7b7-f7be-42d6-b38c-737753b4d9ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>entity  - awww, that's a bummer.  you shoulda ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>entity i dived many times for the ball. manage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>entity no, it's not behaving at all. i'm mad. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet\n",
       "0          0  entity  - awww, that's a bummer.  you shoulda ...\n",
       "1          0  is upset that he can't update his facebook by ...\n",
       "2          0  entity i dived many times for the ball. manage...\n",
       "3          0     my whole body feels itchy and like its on fire\n",
       "4          0  entity no, it's not behaving at all. i'm mad. ..."
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(text_process)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "hcLEQ_Aow5AU"
   },
   "outputs": [],
   "source": [
    "labels = data['Sentiment'].values\n",
    "text = data['Tweet'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Ua2b0VDaw6sw"
   },
   "outputs": [],
   "source": [
    "# import BertTokenizer, convert text into tokens corresponding to BERT lib\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tW1Q4Py7G0Py",
    "outputId": "e48a8b54-b839-41d0-8f51-7525e8406f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  entity  - awww, that's a bummer.  you shoulda got david carr of third day to do it. ;d\n",
      "\n",
      "Tokenized:  ['entity', '-', 'aw', '##w', '##w', ',', 'that', \"'\", 's', 'a', 'bum', '##mer', '.', 'you', 'should', '##a', 'got', 'david', 'carr', 'of', 'third', 'day', 'to', 'do', 'it', '.', ';', 'd']\n",
      "\n",
      "Token IDs:  [9178, 1011, 22091, 2860, 2860, 1010, 2008, 1005, 1055, 1037, 26352, 5017, 1012, 2017, 2323, 2050, 2288, 2585, 12385, 1997, 2353, 2154, 2000, 2079, 2009, 1012, 1025, 1040]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', text[0])\n",
    "print()\n",
    "print('Tokenized: ', tokenizer.tokenize(text[0]))\n",
    "print()\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IB-ji5Mx6bJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# to decide which max_length we are gonna use for padding/truncating\n",
    "# run one full tokenization pass to measure the maximum tweet length\n",
    "max_len = 0\n",
    "\n",
    "for i in text:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens\n",
    "    input_ids = tokenizer.encode(i, add_special_tokens=True)\n",
    "    # Update the max length\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max tweet length: ', max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEkxoiNtHDCs",
    "outputId": "3960f22a-28f6-4482-e1eb-f1311e5e7838"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  entity  - awww, that's a bummer.  you shoulda got david carr of third day to do it. ;d\n",
      "Token IDs: tensor([  101,  9178,  1011, 22091,  2860,  2860,  1010,  2008,  1005,  1055,\n",
      "         1037, 26352,  5017,  1012,  2017,  2323,  2050,  2288,  2585, 12385,\n",
      "         1997,  2353,  2154,  2000,  2079,  2009,  1012,  1025,  1040,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all the texts and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i in text:\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        i,                        \n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,          # Pad & truncate all tweets\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True, # Construct attention masks\n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded tweet to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # Add attention mask \n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Print a tweet as a list of IDs.\n",
    "print('Original: ', text[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIh7sKIhw_ye",
    "outputId": "3232c750-a0cb-469c-faf2-fec1d4d0e866"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,280,000 training samples\n",
      "320,000 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 80-20 train-val split\n",
    "# Number of samples to be included in train/val set\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vVIYGJbMywMn"
   },
   "outputs": [],
   "source": [
    "# batch size for training\n",
    "# the author of the paper recommend 32 or 64\n",
    "# we choose 32 here.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our train/val sets.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset), \n",
    "            batch_size = batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VWJ1BTITy5cc",
    "outputId": "11cd478d-9d9f-4294-9f86-9b440f588612"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# our classification model - BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased',\n",
    "        num_labels = 2,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjQ53kmBHkTQ",
    "outputId": "ace7c15b-753e-45f0-8d43-628248341227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jud5v6tuHsVX"
   },
   "source": [
    "### Optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "B0m2sUmHHmEa"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, \n",
    "                  eps = 1e-8, \n",
    "                  correct_bias=True)\n",
    "\n",
    "# Number of training epochs. \n",
    "epochs = 2\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-JCduXlNHrs2"
   },
   "outputs": [],
   "source": [
    "# Helper Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hkILHRn_3AKe",
    "outputId": "1c4a8ded-66fb-4e11-8d4e-b11a993971a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "rsiZ3nUh7KjH",
    "outputId": "58fa92be-a4c9-49cf-fdc5-b73948794250"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  428781 KB |  428781 KB |  428781 KB |       0 B  |\\n|       from large pool |  428288 KB |  428288 KB |  428288 KB |       0 B  |\\n|       from small pool |     493 KB |     493 KB |     493 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  428781 KB |  428781 KB |  428781 KB |       0 B  |\\n|       from large pool |  428288 KB |  428288 KB |  428288 KB |       0 B  |\\n|       from small pool |     493 KB |     493 KB |     493 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  483328 KB |  483328 KB |  483328 KB |       0 B  |\\n|       from large pool |  481280 KB |  481280 KB |  481280 KB |       0 B  |\\n|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   54546 KB |   54556 KB |  265210 KB |  210663 KB |\\n|       from large pool |   52992 KB |   52992 KB |  263168 KB |  210176 KB |\\n|       from small pool |    1554 KB |    2042 KB |    2042 KB |     487 KB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     202    |     202    |     202    |       0    |\\n|       from large pool |      75    |      75    |      75    |       0    |\\n|       from small pool |     127    |     127    |     127    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     202    |     202    |     202    |       0    |\\n|       from large pool |      75    |      75    |      75    |       0    |\\n|       from small pool |     127    |     127    |     127    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      21    |      21    |      21    |       0    |\\n|       from large pool |      20    |      20    |      20    |       0    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      19    |      19    |      20    |       1    |\\n|       from large pool |      18    |      18    |      19    |       1    |\\n|       from small pool |       1    |       1    |       1    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jb2sUPnKAjJ4",
    "outputId": "362e73ab-a355-4a04-f443-357fb9adcb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      " Batch   200  of  40,000.    Elapsed: 0:02:33.\n",
      " Batch   400  of  40,000.    Elapsed: 0:05:06.\n",
      " Batch   600  of  40,000.    Elapsed: 0:07:39.\n",
      " Batch   800  of  40,000.    Elapsed: 0:10:13.\n",
      " Batch 1,000  of  40,000.    Elapsed: 0:12:46.\n",
      " Batch 1,200  of  40,000.    Elapsed: 0:15:19.\n",
      " Batch 1,400  of  40,000.    Elapsed: 0:17:52.\n",
      " Batch 1,600  of  40,000.    Elapsed: 0:20:25.\n",
      " Batch 1,800  of  40,000.    Elapsed: 0:22:59.\n",
      " Batch 2,000  of  40,000.    Elapsed: 0:25:32.\n",
      " Batch 2,200  of  40,000.    Elapsed: 0:28:05.\n",
      " Batch 2,400  of  40,000.    Elapsed: 0:30:38.\n",
      " Batch 2,600  of  40,000.    Elapsed: 0:33:12.\n",
      " Batch 2,800  of  40,000.    Elapsed: 0:35:45.\n",
      " Batch 3,000  of  40,000.    Elapsed: 0:38:18.\n",
      " Batch 3,200  of  40,000.    Elapsed: 0:40:52.\n",
      " Batch 3,400  of  40,000.    Elapsed: 0:43:25.\n",
      " Batch 3,600  of  40,000.    Elapsed: 0:45:58.\n",
      " Batch 3,800  of  40,000.    Elapsed: 0:48:31.\n",
      " Batch 4,000  of  40,000.    Elapsed: 0:51:05.\n",
      " Batch 4,200  of  40,000.    Elapsed: 0:53:38.\n",
      " Batch 4,400  of  40,000.    Elapsed: 0:56:11.\n",
      " Batch 4,600  of  40,000.    Elapsed: 0:58:45.\n",
      " Batch 4,800  of  40,000.    Elapsed: 1:01:18.\n",
      " Batch 5,000  of  40,000.    Elapsed: 1:03:51.\n",
      " Batch 5,200  of  40,000.    Elapsed: 1:06:24.\n",
      " Batch 5,400  of  40,000.    Elapsed: 1:08:57.\n",
      " Batch 5,600  of  40,000.    Elapsed: 1:11:31.\n",
      " Batch 5,800  of  40,000.    Elapsed: 1:14:04.\n",
      " Batch 6,000  of  40,000.    Elapsed: 1:16:37.\n",
      " Batch 6,200  of  40,000.    Elapsed: 1:19:10.\n",
      " Batch 6,400  of  40,000.    Elapsed: 1:21:43.\n",
      " Batch 6,600  of  40,000.    Elapsed: 1:24:16.\n",
      " Batch 6,800  of  40,000.    Elapsed: 1:26:50.\n",
      " Batch 7,000  of  40,000.    Elapsed: 1:29:23.\n",
      " Batch 7,200  of  40,000.    Elapsed: 1:31:56.\n",
      " Batch 7,400  of  40,000.    Elapsed: 1:34:29.\n",
      " Batch 7,600  of  40,000.    Elapsed: 1:37:03.\n",
      " Batch 7,800  of  40,000.    Elapsed: 1:39:36.\n",
      " Batch 8,000  of  40,000.    Elapsed: 1:42:09.\n",
      " Batch 8,200  of  40,000.    Elapsed: 1:44:43.\n",
      " Batch 8,400  of  40,000.    Elapsed: 1:47:16.\n",
      " Batch 8,600  of  40,000.    Elapsed: 1:49:49.\n",
      " Batch 8,800  of  40,000.    Elapsed: 1:52:22.\n",
      " Batch 9,000  of  40,000.    Elapsed: 1:54:56.\n",
      " Batch 9,200  of  40,000.    Elapsed: 1:57:29.\n",
      " Batch 9,400  of  40,000.    Elapsed: 2:00:02.\n",
      " Batch 9,600  of  40,000.    Elapsed: 2:02:36.\n",
      " Batch 9,800  of  40,000.    Elapsed: 2:05:09.\n",
      " Batch 10,000  of  40,000.    Elapsed: 2:07:42.\n",
      " Batch 10,200  of  40,000.    Elapsed: 2:10:16.\n",
      " Batch 10,400  of  40,000.    Elapsed: 2:12:49.\n",
      " Batch 10,600  of  40,000.    Elapsed: 2:15:22.\n",
      " Batch 10,800  of  40,000.    Elapsed: 2:17:56.\n",
      " Batch 11,000  of  40,000.    Elapsed: 2:20:29.\n",
      " Batch 11,200  of  40,000.    Elapsed: 2:23:02.\n",
      " Batch 11,400  of  40,000.    Elapsed: 2:25:36.\n",
      " Batch 11,600  of  40,000.    Elapsed: 2:28:09.\n",
      " Batch 11,800  of  40,000.    Elapsed: 2:30:43.\n",
      " Batch 12,000  of  40,000.    Elapsed: 2:33:16.\n",
      " Batch 12,200  of  40,000.    Elapsed: 2:35:49.\n",
      " Batch 12,400  of  40,000.    Elapsed: 2:38:23.\n",
      " Batch 12,600  of  40,000.    Elapsed: 2:40:56.\n",
      " Batch 12,800  of  40,000.    Elapsed: 2:43:30.\n",
      " Batch 13,000  of  40,000.    Elapsed: 2:46:03.\n",
      " Batch 13,200  of  40,000.    Elapsed: 2:48:37.\n",
      " Batch 13,400  of  40,000.    Elapsed: 2:51:10.\n",
      " Batch 13,600  of  40,000.    Elapsed: 2:53:44.\n",
      " Batch 13,800  of  40,000.    Elapsed: 2:56:17.\n",
      " Batch 14,000  of  40,000.    Elapsed: 2:58:50.\n",
      " Batch 14,200  of  40,000.    Elapsed: 3:01:24.\n",
      " Batch 14,400  of  40,000.    Elapsed: 3:03:57.\n",
      " Batch 14,600  of  40,000.    Elapsed: 3:06:31.\n",
      " Batch 14,800  of  40,000.    Elapsed: 3:09:04.\n",
      " Batch 15,000  of  40,000.    Elapsed: 3:11:38.\n",
      " Batch 15,200  of  40,000.    Elapsed: 3:14:11.\n",
      " Batch 15,400  of  40,000.    Elapsed: 3:16:44.\n",
      " Batch 15,600  of  40,000.    Elapsed: 3:19:18.\n",
      " Batch 15,800  of  40,000.    Elapsed: 3:21:51.\n",
      " Batch 16,000  of  40,000.    Elapsed: 3:24:24.\n",
      " Batch 16,200  of  40,000.    Elapsed: 3:26:58.\n",
      " Batch 16,400  of  40,000.    Elapsed: 3:29:31.\n",
      " Batch 16,600  of  40,000.    Elapsed: 3:32:04.\n",
      " Batch 16,800  of  40,000.    Elapsed: 3:34:38.\n",
      " Batch 17,000  of  40,000.    Elapsed: 3:37:11.\n",
      " Batch 17,200  of  40,000.    Elapsed: 3:39:45.\n",
      " Batch 17,400  of  40,000.    Elapsed: 3:42:18.\n",
      " Batch 17,600  of  40,000.    Elapsed: 3:44:51.\n",
      " Batch 17,800  of  40,000.    Elapsed: 3:47:25.\n",
      " Batch 18,000  of  40,000.    Elapsed: 3:49:58.\n",
      " Batch 18,200  of  40,000.    Elapsed: 3:52:32.\n",
      " Batch 18,400  of  40,000.    Elapsed: 3:55:05.\n",
      " Batch 18,600  of  40,000.    Elapsed: 3:57:39.\n",
      " Batch 18,800  of  40,000.    Elapsed: 4:00:12.\n",
      " Batch 19,000  of  40,000.    Elapsed: 4:02:46.\n",
      " Batch 19,200  of  40,000.    Elapsed: 4:05:19.\n",
      " Batch 19,400  of  40,000.    Elapsed: 4:07:52.\n",
      " Batch 19,600  of  40,000.    Elapsed: 4:10:26.\n",
      " Batch 19,800  of  40,000.    Elapsed: 4:12:59.\n",
      " Batch 20,000  of  40,000.    Elapsed: 4:15:33.\n",
      " Batch 20,200  of  40,000.    Elapsed: 4:18:06.\n",
      " Batch 20,400  of  40,000.    Elapsed: 4:20:39.\n",
      " Batch 20,600  of  40,000.    Elapsed: 4:23:13.\n",
      " Batch 20,800  of  40,000.    Elapsed: 4:25:46.\n",
      " Batch 21,000  of  40,000.    Elapsed: 4:28:20.\n",
      " Batch 21,200  of  40,000.    Elapsed: 4:30:53.\n",
      " Batch 21,400  of  40,000.    Elapsed: 4:33:26.\n",
      " Batch 21,600  of  40,000.    Elapsed: 4:36:00.\n",
      " Batch 21,800  of  40,000.    Elapsed: 4:38:33.\n",
      " Batch 22,000  of  40,000.    Elapsed: 4:41:07.\n",
      " Batch 22,200  of  40,000.    Elapsed: 4:43:40.\n",
      " Batch 22,400  of  40,000.    Elapsed: 4:46:14.\n",
      " Batch 22,600  of  40,000.    Elapsed: 4:48:47.\n",
      " Batch 22,800  of  40,000.    Elapsed: 4:51:21.\n",
      " Batch 23,000  of  40,000.    Elapsed: 4:53:54.\n",
      " Batch 23,200  of  40,000.    Elapsed: 4:56:28.\n",
      " Batch 23,400  of  40,000.    Elapsed: 4:59:01.\n",
      " Batch 23,600  of  40,000.    Elapsed: 5:01:35.\n",
      " Batch 23,800  of  40,000.    Elapsed: 5:04:08.\n",
      " Batch 24,000  of  40,000.    Elapsed: 5:06:42.\n",
      " Batch 24,200  of  40,000.    Elapsed: 5:09:15.\n",
      " Batch 24,400  of  40,000.    Elapsed: 5:11:49.\n",
      " Batch 24,600  of  40,000.    Elapsed: 5:14:22.\n",
      " Batch 24,800  of  40,000.    Elapsed: 5:16:56.\n",
      " Batch 25,000  of  40,000.    Elapsed: 5:19:29.\n",
      " Batch 25,200  of  40,000.    Elapsed: 5:22:03.\n",
      " Batch 25,400  of  40,000.    Elapsed: 5:24:36.\n",
      " Batch 25,600  of  40,000.    Elapsed: 5:27:10.\n",
      " Batch 25,800  of  40,000.    Elapsed: 5:29:44.\n",
      " Batch 26,000  of  40,000.    Elapsed: 5:32:17.\n",
      " Batch 26,200  of  40,000.    Elapsed: 5:34:51.\n",
      " Batch 26,400  of  40,000.    Elapsed: 5:37:24.\n",
      " Batch 26,600  of  40,000.    Elapsed: 5:39:58.\n",
      " Batch 26,800  of  40,000.    Elapsed: 5:42:31.\n",
      " Batch 27,000  of  40,000.    Elapsed: 5:45:05.\n",
      " Batch 27,200  of  40,000.    Elapsed: 5:47:39.\n",
      " Batch 27,400  of  40,000.    Elapsed: 5:50:12.\n",
      " Batch 27,600  of  40,000.    Elapsed: 5:52:46.\n",
      " Batch 27,800  of  40,000.    Elapsed: 5:55:19.\n",
      " Batch 28,000  of  40,000.    Elapsed: 5:57:53.\n",
      " Batch 28,200  of  40,000.    Elapsed: 6:00:26.\n",
      " Batch 28,400  of  40,000.    Elapsed: 6:03:00.\n",
      " Batch 28,600  of  40,000.    Elapsed: 6:05:34.\n",
      " Batch 28,800  of  40,000.    Elapsed: 6:08:07.\n",
      " Batch 29,000  of  40,000.    Elapsed: 6:10:41.\n",
      " Batch 29,200  of  40,000.    Elapsed: 6:13:14.\n",
      " Batch 29,400  of  40,000.    Elapsed: 6:15:48.\n",
      " Batch 29,600  of  40,000.    Elapsed: 6:18:22.\n",
      " Batch 29,800  of  40,000.    Elapsed: 6:20:55.\n",
      " Batch 30,000  of  40,000.    Elapsed: 6:23:29.\n",
      " Batch 30,200  of  40,000.    Elapsed: 6:26:03.\n",
      " Batch 30,400  of  40,000.    Elapsed: 6:28:36.\n",
      " Batch 30,600  of  40,000.    Elapsed: 6:31:10.\n",
      " Batch 30,800  of  40,000.    Elapsed: 6:33:43.\n",
      " Batch 31,000  of  40,000.    Elapsed: 6:36:17.\n",
      " Batch 31,200  of  40,000.    Elapsed: 6:38:50.\n",
      " Batch 31,400  of  40,000.    Elapsed: 6:41:24.\n",
      " Batch 31,600  of  40,000.    Elapsed: 6:43:57.\n",
      " Batch 31,800  of  40,000.    Elapsed: 6:46:31.\n",
      " Batch 32,000  of  40,000.    Elapsed: 6:49:04.\n",
      " Batch 32,200  of  40,000.    Elapsed: 6:51:37.\n",
      " Batch 32,400  of  40,000.    Elapsed: 6:54:11.\n",
      " Batch 32,600  of  40,000.    Elapsed: 6:56:44.\n",
      " Batch 32,800  of  40,000.    Elapsed: 6:59:18.\n",
      " Batch 33,000  of  40,000.    Elapsed: 7:01:51.\n",
      " Batch 33,200  of  40,000.    Elapsed: 7:04:25.\n",
      " Batch 33,400  of  40,000.    Elapsed: 7:06:58.\n",
      " Batch 33,600  of  40,000.    Elapsed: 7:09:32.\n",
      " Batch 33,800  of  40,000.    Elapsed: 7:12:05.\n",
      " Batch 34,000  of  40,000.    Elapsed: 7:14:38.\n",
      " Batch 34,200  of  40,000.    Elapsed: 7:17:12.\n",
      " Batch 34,400  of  40,000.    Elapsed: 7:19:45.\n",
      " Batch 34,600  of  40,000.    Elapsed: 7:22:19.\n",
      " Batch 34,800  of  40,000.    Elapsed: 7:24:52.\n",
      " Batch 35,000  of  40,000.    Elapsed: 7:27:26.\n",
      " Batch 35,200  of  40,000.    Elapsed: 7:29:59.\n",
      " Batch 35,400  of  40,000.    Elapsed: 7:32:33.\n",
      " Batch 35,600  of  40,000.    Elapsed: 7:35:06.\n",
      " Batch 35,800  of  40,000.    Elapsed: 7:37:40.\n",
      " Batch 36,000  of  40,000.    Elapsed: 7:40:13.\n",
      " Batch 36,200  of  40,000.    Elapsed: 7:42:47.\n",
      " Batch 36,400  of  40,000.    Elapsed: 7:45:20.\n",
      " Batch 36,600  of  40,000.    Elapsed: 7:47:54.\n",
      " Batch 36,800  of  40,000.    Elapsed: 7:50:27.\n",
      " Batch 37,000  of  40,000.    Elapsed: 7:53:01.\n",
      " Batch 37,200  of  40,000.    Elapsed: 7:55:34.\n",
      " Batch 37,400  of  40,000.    Elapsed: 7:58:08.\n",
      " Batch 37,600  of  40,000.    Elapsed: 8:00:41.\n",
      " Batch 37,800  of  40,000.    Elapsed: 8:03:14.\n",
      " Batch 38,000  of  40,000.    Elapsed: 8:05:48.\n",
      " Batch 38,200  of  40,000.    Elapsed: 8:08:21.\n",
      " Batch 38,400  of  40,000.    Elapsed: 8:10:55.\n",
      " Batch 38,600  of  40,000.    Elapsed: 8:13:28.\n",
      " Batch 38,800  of  40,000.    Elapsed: 8:16:01.\n",
      " Batch 39,000  of  40,000.    Elapsed: 8:18:35.\n",
      " Batch 39,200  of  40,000.    Elapsed: 8:21:08.\n",
      " Batch 39,400  of  40,000.    Elapsed: 8:23:42.\n",
      " Batch 39,600  of  40,000.    Elapsed: 8:26:15.\n",
      " Batch 39,800  of  40,000.    Elapsed: 8:28:49.\n",
      "\n",
      "Average training loss: 0.33\n",
      "Training epoch took: 8:31:22\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.87\n",
      "  Validation Loss: 0.30\n",
      "  Validation took: 0:42:20\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      " Batch   200  of  40,000.    Elapsed: 0:02:33.\n",
      " Batch   400  of  40,000.    Elapsed: 0:05:07.\n",
      " Batch   600  of  40,000.    Elapsed: 0:07:40.\n",
      " Batch   800  of  40,000.    Elapsed: 0:10:13.\n",
      " Batch 1,000  of  40,000.    Elapsed: 0:12:46.\n",
      " Batch 1,200  of  40,000.    Elapsed: 0:15:20.\n",
      " Batch 1,400  of  40,000.    Elapsed: 0:17:53.\n",
      " Batch 1,600  of  40,000.    Elapsed: 0:20:26.\n",
      " Batch 1,800  of  40,000.    Elapsed: 0:23:00.\n",
      " Batch 2,000  of  40,000.    Elapsed: 0:25:33.\n",
      " Batch 2,200  of  40,000.    Elapsed: 0:28:06.\n",
      " Batch 2,400  of  40,000.    Elapsed: 0:30:39.\n",
      " Batch 2,600  of  40,000.    Elapsed: 0:33:13.\n",
      " Batch 2,800  of  40,000.    Elapsed: 0:35:46.\n",
      " Batch 3,000  of  40,000.    Elapsed: 0:38:19.\n",
      " Batch 3,200  of  40,000.    Elapsed: 0:40:52.\n",
      " Batch 3,400  of  40,000.    Elapsed: 0:43:26.\n",
      " Batch 3,600  of  40,000.    Elapsed: 0:45:59.\n",
      " Batch 3,800  of  40,000.    Elapsed: 0:48:32.\n",
      " Batch 4,000  of  40,000.    Elapsed: 0:51:06.\n",
      " Batch 4,200  of  40,000.    Elapsed: 0:53:39.\n",
      " Batch 4,400  of  40,000.    Elapsed: 0:56:12.\n",
      " Batch 4,600  of  40,000.    Elapsed: 0:58:46.\n",
      " Batch 4,800  of  40,000.    Elapsed: 1:01:19.\n",
      " Batch 5,000  of  40,000.    Elapsed: 1:03:52.\n",
      " Batch 5,200  of  40,000.    Elapsed: 1:06:25.\n",
      " Batch 5,400  of  40,000.    Elapsed: 1:08:59.\n",
      " Batch 5,600  of  40,000.    Elapsed: 1:11:32.\n",
      " Batch 5,800  of  40,000.    Elapsed: 1:14:05.\n",
      " Batch 6,000  of  40,000.    Elapsed: 1:16:38.\n",
      " Batch 6,200  of  40,000.    Elapsed: 1:19:12.\n",
      " Batch 6,400  of  40,000.    Elapsed: 1:21:45.\n",
      " Batch 6,600  of  40,000.    Elapsed: 1:24:18.\n",
      " Batch 6,800  of  40,000.    Elapsed: 1:26:51.\n",
      " Batch 7,000  of  40,000.    Elapsed: 1:29:25.\n",
      " Batch 7,200  of  40,000.    Elapsed: 1:31:58.\n",
      " Batch 7,400  of  40,000.    Elapsed: 1:34:31.\n",
      " Batch 7,600  of  40,000.    Elapsed: 1:37:05.\n",
      " Batch 7,800  of  40,000.    Elapsed: 1:39:38.\n",
      " Batch 8,000  of  40,000.    Elapsed: 1:42:11.\n",
      " Batch 8,200  of  40,000.    Elapsed: 1:44:44.\n",
      " Batch 8,400  of  40,000.    Elapsed: 1:47:18.\n",
      " Batch 8,600  of  40,000.    Elapsed: 1:49:51.\n",
      " Batch 8,800  of  40,000.    Elapsed: 1:52:24.\n",
      " Batch 9,000  of  40,000.    Elapsed: 1:54:57.\n",
      " Batch 9,200  of  40,000.    Elapsed: 1:57:30.\n",
      " Batch 9,400  of  40,000.    Elapsed: 2:00:04.\n",
      " Batch 9,600  of  40,000.    Elapsed: 2:02:37.\n",
      " Batch 9,800  of  40,000.    Elapsed: 2:05:10.\n",
      " Batch 10,000  of  40,000.    Elapsed: 2:07:44.\n",
      " Batch 10,200  of  40,000.    Elapsed: 2:10:17.\n",
      " Batch 10,400  of  40,000.    Elapsed: 2:12:50.\n",
      " Batch 10,600  of  40,000.    Elapsed: 2:15:23.\n",
      " Batch 10,800  of  40,000.    Elapsed: 2:17:56.\n",
      " Batch 11,000  of  40,000.    Elapsed: 2:20:30.\n",
      " Batch 11,200  of  40,000.    Elapsed: 2:23:03.\n",
      " Batch 11,400  of  40,000.    Elapsed: 2:25:36.\n",
      " Batch 11,600  of  40,000.    Elapsed: 2:28:09.\n",
      " Batch 11,800  of  40,000.    Elapsed: 2:30:43.\n",
      " Batch 12,000  of  40,000.    Elapsed: 2:33:16.\n",
      " Batch 12,200  of  40,000.    Elapsed: 2:35:49.\n",
      " Batch 12,400  of  40,000.    Elapsed: 2:38:22.\n",
      " Batch 12,600  of  40,000.    Elapsed: 2:40:56.\n",
      " Batch 12,800  of  40,000.    Elapsed: 2:43:29.\n",
      " Batch 13,000  of  40,000.    Elapsed: 2:46:02.\n",
      " Batch 13,200  of  40,000.    Elapsed: 2:48:36.\n",
      " Batch 13,400  of  40,000.    Elapsed: 2:51:09.\n",
      " Batch 13,600  of  40,000.    Elapsed: 2:53:42.\n",
      " Batch 13,800  of  40,000.    Elapsed: 2:56:16.\n",
      " Batch 14,000  of  40,000.    Elapsed: 2:58:49.\n",
      " Batch 14,200  of  40,000.    Elapsed: 3:01:22.\n",
      " Batch 14,400  of  40,000.    Elapsed: 3:03:55.\n",
      " Batch 14,600  of  40,000.    Elapsed: 3:06:29.\n",
      " Batch 14,800  of  40,000.    Elapsed: 3:09:02.\n",
      " Batch 15,000  of  40,000.    Elapsed: 3:11:35.\n",
      " Batch 15,200  of  40,000.    Elapsed: 3:14:08.\n",
      " Batch 15,400  of  40,000.    Elapsed: 3:16:41.\n",
      " Batch 15,600  of  40,000.    Elapsed: 3:19:14.\n",
      " Batch 15,800  of  40,000.    Elapsed: 3:21:48.\n",
      " Batch 16,000  of  40,000.    Elapsed: 3:24:21.\n",
      " Batch 16,200  of  40,000.    Elapsed: 3:26:54.\n",
      " Batch 16,400  of  40,000.    Elapsed: 3:29:27.\n",
      " Batch 16,600  of  40,000.    Elapsed: 3:32:00.\n",
      " Batch 16,800  of  40,000.    Elapsed: 3:34:33.\n",
      " Batch 17,000  of  40,000.    Elapsed: 3:37:06.\n",
      " Batch 17,200  of  40,000.    Elapsed: 3:39:39.\n",
      " Batch 17,400  of  40,000.    Elapsed: 3:42:12.\n",
      " Batch 17,600  of  40,000.    Elapsed: 3:44:46.\n",
      " Batch 17,800  of  40,000.    Elapsed: 3:47:19.\n",
      " Batch 18,000  of  40,000.    Elapsed: 3:49:52.\n",
      " Batch 18,200  of  40,000.    Elapsed: 3:52:25.\n",
      " Batch 18,400  of  40,000.    Elapsed: 3:54:58.\n",
      " Batch 18,600  of  40,000.    Elapsed: 3:57:31.\n",
      " Batch 18,800  of  40,000.    Elapsed: 4:00:04.\n",
      " Batch 19,000  of  40,000.    Elapsed: 4:02:38.\n",
      " Batch 19,200  of  40,000.    Elapsed: 4:05:11.\n",
      " Batch 19,400  of  40,000.    Elapsed: 4:07:44.\n",
      " Batch 19,600  of  40,000.    Elapsed: 4:10:17.\n",
      " Batch 19,800  of  40,000.    Elapsed: 4:12:50.\n",
      " Batch 20,000  of  40,000.    Elapsed: 4:15:23.\n",
      " Batch 20,200  of  40,000.    Elapsed: 4:17:56.\n",
      " Batch 20,400  of  40,000.    Elapsed: 4:20:29.\n",
      " Batch 20,600  of  40,000.    Elapsed: 4:23:03.\n",
      " Batch 20,800  of  40,000.    Elapsed: 4:25:36.\n",
      " Batch 21,000  of  40,000.    Elapsed: 4:28:09.\n",
      " Batch 21,200  of  40,000.    Elapsed: 4:30:42.\n",
      " Batch 21,400  of  40,000.    Elapsed: 4:33:15.\n",
      " Batch 21,600  of  40,000.    Elapsed: 4:35:48.\n",
      " Batch 21,800  of  40,000.    Elapsed: 4:38:22.\n",
      " Batch 22,000  of  40,000.    Elapsed: 4:40:55.\n",
      " Batch 22,200  of  40,000.    Elapsed: 4:43:28.\n",
      " Batch 22,400  of  40,000.    Elapsed: 4:46:01.\n",
      " Batch 22,600  of  40,000.    Elapsed: 4:48:34.\n",
      " Batch 22,800  of  40,000.    Elapsed: 4:51:07.\n",
      " Batch 23,000  of  40,000.    Elapsed: 4:53:40.\n",
      " Batch 23,200  of  40,000.    Elapsed: 4:56:14.\n",
      " Batch 23,400  of  40,000.    Elapsed: 4:58:47.\n",
      " Batch 23,600  of  40,000.    Elapsed: 5:01:20.\n",
      " Batch 23,800  of  40,000.    Elapsed: 5:03:53.\n",
      " Batch 24,000  of  40,000.    Elapsed: 5:06:26.\n",
      " Batch 24,200  of  40,000.    Elapsed: 5:08:59.\n",
      " Batch 24,400  of  40,000.    Elapsed: 5:11:33.\n",
      " Batch 24,600  of  40,000.    Elapsed: 5:14:06.\n",
      " Batch 24,800  of  40,000.    Elapsed: 5:16:39.\n",
      " Batch 25,000  of  40,000.    Elapsed: 5:19:12.\n",
      " Batch 25,200  of  40,000.    Elapsed: 5:21:45.\n",
      " Batch 25,400  of  40,000.    Elapsed: 5:24:18.\n",
      " Batch 25,600  of  40,000.    Elapsed: 5:26:52.\n",
      " Batch 25,800  of  40,000.    Elapsed: 5:29:25.\n",
      " Batch 26,000  of  40,000.    Elapsed: 5:31:58.\n",
      " Batch 26,200  of  40,000.    Elapsed: 5:34:31.\n",
      " Batch 26,400  of  40,000.    Elapsed: 5:37:04.\n",
      " Batch 26,600  of  40,000.    Elapsed: 5:39:37.\n",
      " Batch 26,800  of  40,000.    Elapsed: 5:42:10.\n",
      " Batch 27,000  of  40,000.    Elapsed: 5:44:43.\n",
      " Batch 27,200  of  40,000.    Elapsed: 5:47:17.\n",
      " Batch 27,400  of  40,000.    Elapsed: 5:49:50.\n",
      " Batch 27,600  of  40,000.    Elapsed: 5:52:23.\n",
      " Batch 27,800  of  40,000.    Elapsed: 5:54:56.\n",
      " Batch 28,000  of  40,000.    Elapsed: 5:57:29.\n",
      " Batch 28,200  of  40,000.    Elapsed: 6:00:02.\n",
      " Batch 28,400  of  40,000.    Elapsed: 6:02:36.\n",
      " Batch 28,600  of  40,000.    Elapsed: 6:05:09.\n",
      " Batch 28,800  of  40,000.    Elapsed: 6:07:42.\n",
      " Batch 29,000  of  40,000.    Elapsed: 6:10:15.\n",
      " Batch 29,200  of  40,000.    Elapsed: 6:12:48.\n",
      " Batch 29,400  of  40,000.    Elapsed: 6:15:22.\n",
      " Batch 29,600  of  40,000.    Elapsed: 6:17:55.\n",
      " Batch 29,800  of  40,000.    Elapsed: 6:20:28.\n",
      " Batch 30,000  of  40,000.    Elapsed: 6:23:01.\n",
      " Batch 30,200  of  40,000.    Elapsed: 6:25:34.\n",
      " Batch 30,400  of  40,000.    Elapsed: 6:28:07.\n",
      " Batch 30,600  of  40,000.    Elapsed: 6:30:41.\n",
      " Batch 30,800  of  40,000.    Elapsed: 6:33:14.\n",
      " Batch 31,000  of  40,000.    Elapsed: 6:35:47.\n",
      " Batch 31,200  of  40,000.    Elapsed: 6:38:20.\n",
      " Batch 31,400  of  40,000.    Elapsed: 6:40:53.\n",
      " Batch 31,600  of  40,000.    Elapsed: 6:43:27.\n",
      " Batch 31,800  of  40,000.    Elapsed: 6:46:00.\n",
      " Batch 32,000  of  40,000.    Elapsed: 6:48:33.\n",
      " Batch 32,200  of  40,000.    Elapsed: 6:51:06.\n",
      " Batch 32,400  of  40,000.    Elapsed: 6:53:39.\n",
      " Batch 32,600  of  40,000.    Elapsed: 6:56:13.\n",
      " Batch 32,800  of  40,000.    Elapsed: 6:58:46.\n",
      " Batch 33,000  of  40,000.    Elapsed: 7:01:19.\n",
      " Batch 33,200  of  40,000.    Elapsed: 7:03:52.\n",
      " Batch 33,400  of  40,000.    Elapsed: 7:06:25.\n",
      " Batch 33,600  of  40,000.    Elapsed: 7:08:59.\n",
      " Batch 33,800  of  40,000.    Elapsed: 7:11:32.\n",
      " Batch 34,000  of  40,000.    Elapsed: 7:14:05.\n",
      " Batch 34,200  of  40,000.    Elapsed: 7:16:38.\n",
      " Batch 34,400  of  40,000.    Elapsed: 7:19:11.\n",
      " Batch 34,600  of  40,000.    Elapsed: 7:21:44.\n",
      " Batch 34,800  of  40,000.    Elapsed: 7:24:18.\n",
      " Batch 35,000  of  40,000.    Elapsed: 7:26:51.\n",
      " Batch 35,200  of  40,000.    Elapsed: 7:29:24.\n",
      " Batch 35,400  of  40,000.    Elapsed: 7:31:57.\n",
      " Batch 35,600  of  40,000.    Elapsed: 7:34:30.\n",
      " Batch 35,800  of  40,000.    Elapsed: 7:37:04.\n",
      " Batch 36,000  of  40,000.    Elapsed: 7:39:37.\n",
      " Batch 36,200  of  40,000.    Elapsed: 7:42:10.\n",
      " Batch 36,400  of  40,000.    Elapsed: 7:44:43.\n",
      " Batch 36,600  of  40,000.    Elapsed: 7:47:17.\n",
      " Batch 36,800  of  40,000.    Elapsed: 7:49:50.\n",
      " Batch 37,000  of  40,000.    Elapsed: 7:52:23.\n",
      " Batch 37,200  of  40,000.    Elapsed: 7:54:56.\n",
      " Batch 37,400  of  40,000.    Elapsed: 7:57:30.\n",
      " Batch 37,600  of  40,000.    Elapsed: 8:00:03.\n",
      " Batch 37,800  of  40,000.    Elapsed: 8:02:36.\n",
      " Batch 38,000  of  40,000.    Elapsed: 8:05:09.\n",
      " Batch 38,200  of  40,000.    Elapsed: 8:07:43.\n",
      " Batch 38,400  of  40,000.    Elapsed: 8:10:16.\n",
      " Batch 38,600  of  40,000.    Elapsed: 8:12:49.\n",
      " Batch 38,800  of  40,000.    Elapsed: 8:15:22.\n",
      " Batch 39,000  of  40,000.    Elapsed: 8:17:56.\n",
      " Batch 39,200  of  40,000.    Elapsed: 8:20:29.\n",
      " Batch 39,400  of  40,000.    Elapsed: 8:23:02.\n",
      " Batch 39,600  of  40,000.    Elapsed: 8:25:35.\n",
      " Batch 39,800  of  40,000.    Elapsed: 8:28:09.\n",
      "\n",
      "Average training loss: 0.26\n",
      "Training epoch took: 8:30:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.88\n",
      "  Validation Loss: 0.30\n",
      "  Validation took: 0:42:19\n",
      "\n",
      "Training completed!\n",
      "Total training took 18:26:44 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Referring: 'run_glue.py':\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 138\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store  training and validation loss, validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    #               Training\n",
    "    \n",
    "    # Perform one full epoch over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(' Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        result = model(batch[0].to(device), \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=batch[1].to(device), \n",
    "                       labels=batch[2].to(device),\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Set the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update optimizer parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"Training epoch took: {:}\".format(training_time))\n",
    "        \n",
    "    #               Validation\n",
    "    \n",
    "    # After training each epoch, measure performance on validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "     \n",
    "        with torch.no_grad():        \n",
    "            result = model(batch[0].to(device), \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=batch[1].to(device),\n",
    "                           labels=batch[2].to(device),\n",
    "                           return_dict=True)\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch[2].to(device).to('cpu').numpy()\n",
    "        \n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Average validation accuracy \n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Validation Time\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training completed!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "RIeR_T4HFXRz",
    "outputId": "a4a07ee8-d0e8-4559-bf78-d583a9199e13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. Accur.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8:31:22</td>\n",
       "      <td>0:42:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.88</td>\n",
       "      <td>8:30:42</td>\n",
       "      <td>0:42:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "1               0.33          0.3           0.87       8:31:22         0:42:20\n",
       "2               0.26          0.3           0.88       8:30:42         0:42:19"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 2)\n",
    "stats = pd.DataFrame(data=training_stats)\n",
    "stats = stats.set_index('epoch')\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTQLmdtIXiG_"
   },
   "source": [
    "### Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tvJmJNnXevO",
    "outputId": "4e722321-b185-4602-c972-52aa5881c5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(498, 6)\n",
      "   Sentiment                                              Tweet\n",
      "0          1  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
      "1          1  Reading my kindle2...  Love it... Lee childs i...\n",
      "2          1  Ok, first assesment of the #kindle2 ...it fuck...\n",
      "3          1  @kenburbary You'll love your Kindle2. I've had...\n",
      "4          1  @mikefish  Fair enough. But i have the Kindle2...\n",
      "Number of test tweets: 498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "test_data = pd.read_csv('/content/drive/My Drive/testdata.manual.2009.06.14.csv',  encoding = 'latin-1', header=None)\n",
    "print(test_data.shape)\n",
    "test_data = test_data[[0, 5]]\n",
    "test_data.columns = ['Sentiment', 'Tweet']\n",
    "test_data.loc[test_data[\"Sentiment\"] == 4, \"Sentiment\"] = 1\n",
    "print(test_data.head())\n",
    "\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test tweets: {:,}\\n'.format(test_data.shape[0]))\n",
    "\n",
    "# preprocess the test data:\n",
    "# clean the dataset\n",
    "import re\n",
    "#remove hashtags\n",
    "hashtags = re.compile(r\"^#\\S+|\\s#\\S+\")\n",
    "#remove @ mentions \n",
    "mentions = re.compile(r\"^@\\S+|\\s@\\S+\")\n",
    "#remove urls\n",
    "urls = re.compile(r\"https?://\\S+\")\n",
    "\n",
    "def text_process(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = hashtags.sub(' hashtag', text)\n",
    "    text = mentions.sub(' entity', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "test_data['Tweet'] = test_data.Tweet.apply(text_process)\n",
    "test_data.head() \n",
    "\n",
    "\n",
    "# Create text and label lists\n",
    "labels = test_data['Sentiment'].values\n",
    "text = test_data['Tweet'].values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i in text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        i,                     \n",
    "                        add_special_tokens = True, \n",
    "                        max_length = 256,           \n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',     \n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgCYi9XaajxZ",
    "outputId": "ba93aad6-f89e-43c7-bc29-63da9d0f6a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 498 test tweets...\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "print('Predicting labels for {:,} test tweets...'.format(len(input_ids)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(batch[0], \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=batch[1],\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = batch[2].to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIr3z75YpkHS",
    "outputId": "8f878b32-4cc4-4fbd-a273-38f314e27e3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "output_dir = './model_save/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  \n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oqXSdEGd7tEm",
    "outputId": "f120f20c-ec13-4133-ff45-7a2d864ffc53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "import torch\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "output_dir = './'\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPrsjWHQWg2e",
    "outputId": "e24b6968-ee35-49c0-8d16-213189ad2f3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore the model layers\n",
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "NTbTCpM1AIOg"
   },
   "outputs": [],
   "source": [
    "def Sentiment(sent):\n",
    "    output_dir = './'\n",
    "    tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "    model_loaded = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent, \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 64,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "        \n",
    "    input_id = encoded_dict['input_ids']\n",
    "\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    input_id = torch.LongTensor(input_id)\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "\n",
    "    model_loaded = model_loaded.to(device)\n",
    "    input_id = input_id.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    index = logits.argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5VEApZqAJnz",
    "outputId": "57f4bec1-7ded-4555-a4c2-6f0fda1af102"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "ans = Sentiment('Baby Yoda is so cuteeee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4Mr7XiTAKyK",
    "outputId": "89fcb4f7-2a94-4367-b2e8-31d5bc56c4aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "if ans == 1:\n",
    "    print(\"Positive\")\n",
    "else:\n",
    "    print(\"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prl_gwQFAMVS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "BERT Fine-tuning with Sentiment140 data",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
